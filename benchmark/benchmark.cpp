#include "pipeline/ocr_pipeline.h"
#include "common/logger.hpp"
#include "common/visualizer.h"
#include <opencv2/opencv.hpp>
#include <nlohmann/json.hpp>
#include <filesystem>
#include <fstream>
#include <iomanip>
#include <algorithm>
#include <chrono>
#include <numeric>
#include <thread>
#include <atomic>

namespace fs = std::filesystem;
using json = nlohmann::json;

int main(int argc, char** argv) {
    // è§£æå‘½ä»¤è¡Œå‚æ•°ï¼šè¿è¡Œæ¬¡æ•°ã€æ¨¡å‹ç±»å‹ã€æ˜¯å¦ä½¿ç”¨UVDoc
    int runsPerImage = 3;
    std::string modelType = "server";  // Default: server models
    bool useUVDoc = false;  // Default: disable UVDoc for accurate benchmark with ground truth labels
    
    if (argc > 1) {
        runsPerImage = std::atoi(argv[1]);
        if (runsPerImage < 1) runsPerImage = 3;
    }
    
    if (argc > 2) {
        modelType = argv[2];
        if (modelType != "server" && modelType != "mobile") {
            LOG_ERROR("Invalid model type: {}. Use 'server' or 'mobile'", modelType);
            return -1;
        }
    }
    
    if (argc > 3) {
        std::string uvdocArg = argv[3];
        if (uvdocArg == "uvdoc" || uvdocArg == "true" || uvdocArg == "1") {
            useUVDoc = true;
        }
    }
    
    LOG_INFO("========================================");
    LOG_INFO("DeepX OCR - Benchmark (Async Mode)");
    LOG_INFO("========================================\n");
    LOG_INFO("Model Type: {}", modelType);
    LOG_INFO("Use UVDoc: {}", useUVDoc ? "Yes" : "No");
    
    std::string projectRoot = PROJECT_ROOT_DIR;
    std::string imagesDir = projectRoot + "/images";
    std::string outputDir = projectRoot + "/benchmark/results_" + modelType;
    std::string visDir = projectRoot + "/benchmark/vis_" + modelType;
    
    fs::create_directories(outputDir);
    fs::create_directories(visDir);
    
    LOG_INFO("ğŸ“‚ Images: {}", imagesDir);
    LOG_INFO("ğŸ“‚ Output: {}", outputDir);
    LOG_INFO("ğŸ“‚ Visualization: {}", visDir);
    LOG_INFO("ğŸ”„ Runs per image: {}\n", runsPerImage);
    
    // é…ç½®Pipeline - æ ¹æ®æ¨¡å‹ç±»å‹é…ç½®
    ocr::OCRPipelineConfig config;
    
    // è®¾ç½®æ˜¯å¦ä½¿ç”¨mobileæ¨¡å‹
    bool useMobileModel = (modelType == "mobile");
    config.detectorConfig.useMobileModel = useMobileModel;
    config.recognizerConfig.useMobileModel = useMobileModel;
    
    // å¦‚æœä½¿ç”¨mobileæ¨¡å‹ï¼Œæ›´æ–°æ¨¡å‹è·¯å¾„
    if (useMobileModel) {
        std::string modelRoot = projectRoot + "/engine/model_files/mobile";
        config.detectorConfig.model640Path = modelRoot + "/det_mobile_640.dxnn";
        config.detectorConfig.model960Path = modelRoot + "/det_mobile_960.dxnn";
        
        // æ›´æ–°Recognitionæ¨¡å‹è·¯å¾„
        config.recognizerConfig.modelPaths = {
            {3, modelRoot + "/rec_mobile_ratio_3.dxnn"},
            {5, modelRoot + "/rec_mobile_ratio_5.dxnn"},
            {10, modelRoot + "/rec_mobile_ratio_10.dxnn"},
            {15, modelRoot + "/rec_mobile_ratio_15.dxnn"},
            {25, modelRoot + "/rec_mobile_ratio_25.dxnn"},
            {35, modelRoot + "/rec_mobile_ratio_35.dxnn"}
        };
        LOG_INFO("âœ“ Using mobile models\n");
    } else {
        LOG_INFO("âœ“ Using server models\n");
    }
    
    // é…ç½® Document Preprocessing (ä¸ Python demo ä¸€è‡´)
    std::string serverModelRoot = projectRoot + "/engine/model_files/server";
    config.useDocPreprocessing = true;
    config.docPreprocessingConfig.useOrientation = true;
    config.docPreprocessingConfig.orientationConfig.modelPath = serverModelRoot + "/doc_ori_fixed.dxnn";
    config.docPreprocessingConfig.useUnwarping = useUVDoc;  // ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶
    config.docPreprocessingConfig.uvdocConfig.modelPath = serverModelRoot + "/UVDoc_pruned_p3.dxnn";
    config.docPreprocessingConfig.uvdocConfig.inputWidth = 488;
    config.docPreprocessingConfig.uvdocConfig.inputHeight = 712;
    config.docPreprocessingConfig.uvdocConfig.alignCorners = true;
    
    // é…ç½® Classification (ä¸ Python demo ä¸€è‡´)
    config.useClassification = true;
    config.classifierConfig.modelPath = serverModelRoot + "/textline_ori.dxnn";
    config.classifierConfig.threshold = 0.9;
    
    // ç¦ç”¨å¯è§†åŒ–ä»¥æé«˜æ€§èƒ½
    config.enableVisualization = false;
    
    // åˆå§‹åŒ–
    ocr::OCRPipeline pipeline(config);
    if (!pipeline.initialize()) {
        LOG_ERROR("Failed to initialize pipeline");
        return -1;
    }
    
    LOG_INFO("âœ… Pipeline initialized\n");
    
    // è·å–å›¾ç‰‡åˆ—è¡¨
    std::vector<std::string> imageFiles;
    for (const auto& entry : fs::directory_iterator(imagesDir)) {
        if (entry.is_regular_file()) {
            std::string filename = entry.path().filename().string();
            size_t len = filename.length();
            if ((len > 4 && filename.substr(len - 4) == ".png") ||
                (len > 4 && filename.substr(len - 4) == ".jpg")) {
                imageFiles.push_back(entry.path().string());
            }
        }
    }
    std::sort(imageFiles.begin(), imageFiles.end());
    
    if (imageFiles.empty()) {
        LOG_ERROR("No images found in {}", imagesDir);
        return -1;
    }
    
    LOG_INFO("Found {} images\n", imageFiles.size());
    
    // é¢„åŠ è½½æ‰€æœ‰å›¾ç‰‡
    std::vector<cv::Mat> images;
    std::vector<std::string> imageNames;
    images.reserve(imageFiles.size());
    imageNames.reserve(imageFiles.size());
    
    for (const auto& imagePath : imageFiles) {
        cv::Mat image = cv::imread(imagePath);
        if (!image.empty()) {
            images.push_back(image);
            imageNames.push_back(fs::path(imagePath).filename().string());
        }
    }
    
    LOG_INFO("Loaded {} images into memory\n", images.size());
    
    // å¯åŠ¨å¼‚æ­¥ Pipeline
    pipeline.start();
    
    int totalTasks = static_cast<int>(images.size()) * runsPerImage;
    std::atomic<int> completedCount{0};
    
    // å­˜å‚¨æ¯å¼ å›¾ç‰‡çš„ç»“æœ
    std::map<int64_t, std::vector<ocr::PipelineOCRResult>> allResults;
    std::map<int64_t, cv::Mat> processedImages;  // å­˜å‚¨å¤„ç†åçš„å›¾åƒç”¨äºå¯è§†åŒ–
    std::mutex resultsMutex;
    
    auto startTime = std::chrono::high_resolution_clock::now();
    
    // æ¶ˆè´¹è€…çº¿ç¨‹ï¼šæ¥æ”¶ç»“æœ
    std::thread consumer([&]() {
        while (completedCount.load() < totalTasks) {
            std::vector<ocr::PipelineOCRResult> results;
            cv::Mat processedImage;
            int64_t id;
            if (pipeline.getResult(results, id, &processedImage)) {
                {
                    std::lock_guard<std::mutex> lock(resultsMutex);
                    // åªä¿å­˜æœ€åä¸€æ¬¡è¿è¡Œçš„ç»“æœ
                    int imageIdx = id % images.size();
                    int runIdx = id / images.size();
                    
                    LOG_INFO("Got result: id={}, imageIdx={}, runIdx={}, results={}", 
                             id, imageIdx, runIdx, results.size());
                    
                    if (runIdx == runsPerImage - 1) {
                        allResults[imageIdx] = std::move(results);
                        if (!processedImage.empty()) {
                            processedImages[imageIdx] = processedImage.clone();
                        }
                    }
                }
                completedCount.fetch_add(1);
                if (completedCount.load() % 10 == 0) {
                    LOG_INFO("Processed {}/{}", completedCount.load(), totalTasks);
                }
            } else {
                std::this_thread::yield();
            }
        }
    });
    
    // ç”Ÿäº§è€…ï¼šæäº¤æ‰€æœ‰ä»»åŠ¡
    for (int run = 0; run < runsPerImage; ++run) {
        for (size_t i = 0; i < images.size(); ++i) {
            int64_t taskId = run * images.size() + i;
            while (!pipeline.pushTask(images[i], taskId)) {
                std::this_thread::sleep_for(std::chrono::milliseconds(1));
            }
        }
    }
    
    consumer.join();
    auto endTime = std::chrono::high_resolution_clock::now();
    
    pipeline.stop();
    
    double totalTimeMs = std::chrono::duration<double, std::milli>(endTime - startTime).count();
    double avgTimePerImage = totalTimeMs / totalTasks;
    double fps = totalTasks / (totalTimeMs / 1000.0);
    
    LOG_INFO("\n========== Benchmark Results ==========");
    LOG_INFO("Total Tasks: {} (Images: {}, Repeats: {})", totalTasks, images.size(), runsPerImage);
    LOG_INFO("Total Time: {:.2f} ms", totalTimeMs);
    LOG_INFO("Average Time: {:.2f} ms/image", avgTimePerImage);
    LOG_INFO("FPS: {:.2f}", fps);
    LOG_INFO("========================================\n");
    
    // ä¿å­˜æ¯å¼ å›¾ç‰‡çš„è¯¦ç»†ç»“æœ
    std::string fontPath = projectRoot + "/engine/fonts/NotoSansCJK-Regular.ttc";
    int successCount = 0;
    
    for (size_t i = 0; i < images.size(); ++i) {
        auto it = allResults.find(i);
        if (it == allResults.end()) continue;
        
        const auto& results = it->second;
        const std::string& imageName = imageNames[i];
        
        // æ„å»ºJSONè¾“å‡º
        json output;
        std::vector<std::string> rec_texts;
        std::vector<float> rec_scores;
        
        for (const auto& result : results) {
            rec_texts.push_back(result.text);
            rec_scores.push_back(result.confidence);
        }
        
        int totalChars = std::accumulate(rec_texts.begin(), rec_texts.end(), 0,
            [](int sum, const std::string& s) { return sum + static_cast<int>(s.length()); });
        
        output["rec_texts"] = rec_texts;
        output["rec_scores"] = rec_scores;
        output["filename"] = imageName;
        output["total_chars"] = totalChars;
        output["runs"] = runsPerImage;
        output["avg_inference_ms"] = avgTimePerImage;
        output["fps"] = fps;
        output["chars_per_second"] = totalChars * 1000.0 / avgTimePerImage;
        
        // ä¿å­˜JSON
        std::string basePath = outputDir + "/" + 
            imageName.substr(0, imageName.find_last_of('.'));
        std::string jsonPath = basePath + "_result.json";
        std::ofstream jsonFile(jsonPath);
        jsonFile << output.dump(4);
        jsonFile.close();
        
        // ç”Ÿæˆå¯è§†åŒ–
        std::vector<DeepXOCR::TextBox> boxes;
        for (const auto& result : results) {
            DeepXOCR::TextBox box;
            for (int j = 0; j < 4; j++) {
                box.points[j] = result.box[j];
            }
            box.text = result.text;
            box.confidence = result.confidence;
            boxes.push_back(box);
        }
        
        // ä½¿ç”¨å¤„ç†åçš„å›¾åƒè¿›è¡Œå¯è§†åŒ–ï¼ˆå¦‚æœæœ‰UVDocé¢„å¤„ç†ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨åŸå›¾
        cv::Mat imageForVis = processedImages.count(i) ? processedImages[i] : images[i];
        cv::Mat visResult = ocr::Visualizer::drawOCRResultsSideBySide(imageForVis, boxes, fontPath);
        std::string visPath = visDir + "/" + imageName;
        cv::imwrite(visPath, visResult);
        
        successCount++;
    }
    
    LOG_INFO("Completed: {}/{} images", successCount, images.size());
    LOG_INFO("ğŸ“Š Results saved to: {}", outputDir);
    LOG_INFO("ğŸ–¼ï¸  Visualizations saved to: {}", visDir);
    LOG_INFO("\nğŸ’¡ To calculate accuracy and generate full report, run:");
    LOG_INFO("   cd {} && python3 benchmark/run_benchmark.py --no-cpp", projectRoot);
    
    return 0;
}
